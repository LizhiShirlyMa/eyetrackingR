---
title: "A Brief Overview of EyetrackingR"
author: "Jacob Dink & Brock Ferguson"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This vignette will use the entire *eyetrackingR* toolbox to analyze real data from a simple 2-alternative forced choice (2AFC) word recognition task administered to 19- and 24-month-olds.

On each trial, infants were shown a picture of an animate object (e.g., a horse) and an inanimate object (e.g., a spoon). After inspecting the images, they disappeared and they heard a label referring to one of them (e.g., "The horse is nearby!"). Finally, the objects re-appeared on the screen and they were prompted to look at the target (e.g., "Look at the horse!").

We want to ask whether infants looked to the named target. That is, when the animate was named, did they look at the animate? And, when the inanimate was named, did they look at the inanimate?

## Data Preparation

### Load dataset and dependencies, set data options for **eyetrackingR** library.

EyetrackingR only requires five types of columns in your dataset:

* Participant Column : This column specifies the participant code
* Trial Column : This column specifies a trial identifier that is unique for each participant
* Time Column : This column specifies the time-within-trial. If you have a timestamp column, but the beginning of the timestamp doesn't correspond to the beginning of the trial in the way you'd like, the function `subset_by_window` can help fix this.
* Trackloss Column : This column specifies trackloss
* AOI Column(s) : These columns specify whether the gaze is in a particular 'area-of-interest.' Each AOI should have a corresponding column. The elements of this column specify, for every sample, whether the gaze was in that AOI. If you don't have columns like this, the function `add_aoi` can create them.

As long as your dataset has these five types of columns, you are ready to use eyetrackingR for cleaning, visualization, and analysis!

There are also some optional columns:

* Item Column(s) : This corresponds to any 'items' in your experiment: types of stimuli presented across trials. Unlike the 'trial' column, this does not need to be unique. This is often needed for certain types of analyses (e.g., mixed-effects models), but you may not need it for your experiment.
* Predictor Column(s) : These are columns specifying predictors (e.g., column(s) specifying condition). Unlike the types above, these are specified separately for each analysis (and not within `set_data_options` at the outset).

```{r results='hide'}
set.seed(42)
library("Matrix")
library("lme4")
library("ggplot2")
library("eyetrackingR")
data("word_recognition")

# set data options
data_options = set_data_options( 
  participant_column = "ParticipantName",
  trial_column = "Trial",
  time_column = "TimeFromTrialOnset",
  trackloss_column = "TrackLoss",
  aoi_columns = c('Animate','Inanimate')
)
```

### Verify state of each relevant column.

If any of our columns are improperly formatted, this function will (1) notify us and, (2) attempt to fix it.

```{r, warning=FALSE}
data <- verify_dataset(word_recognition, data_options)
```

### Subset data to examine "response" window, and deal with trackloss.

When preparing our data for analysis, we typically want to focus on a particular window in the trial. 

* If we're lucky, this just means using the same start and end time for each trial. 
* If we're unlucky, this means using unique stimulus-presentation timestamps for each trial. 

EyetrackingR lets us handle both types of data. However, our dataset below is the 'lucky' version. 

Here, we subset the data to focus on our time window of interest (a "response window" beginning 500ms pre-word-onset, 15500ms after the start of the trial, and ending at the end of the trial 5500ms later)

```{r, warning=FALSE}
# subset to response window
response_window <- subset_by_window(data, data_options, 
                                    window_start_time = 15500, 
                                    window_end_time = 21000, 
                                    rezero = FALSE)
```

Next, we treat looks outside of our AOIs as if they are trackloss (we don't care if infants looked at the screen unless they were looking at one of our AOIs).

```{r, warning=FALSE}
# convert non-AOI looks to trackloss
response_window <- convert_non_aoi_to_trackloss(response_window, data_options)
```

Next, we...

* Calculate the amount of trackloss in each trial
* Remove trials with over 50% trackloss

```{r, warning=FALSE}
# analyze amount of trackloss by subjects and trials
(trackloss <- trackloss_analysis(data = response_window,
                                data_options = data_options))

response_window_clean <- clean_by_trackloss(data = response_window,
                                            data_options = data_options, 
                                            trial_prop_thresh = .25)
```

Finally, we remove all remaining trackloss samples from our dataset (so that, in each sample, the infant is looking either to one AOI or the other)

```{r, warning=FALSE}
response_window_clean <- remove_trackloss(response_window_clean, data_options, delete_rows=TRUE)
```

### Create "Target" condition based on TrialName

Each trial involved either an animate or inanimate; here we create a column specifying which for each column.

```{r, warning=FALSE}
response_window_clean$Target <- as.factor( ifelse(test = grepl('(Spoon|Bottle)', response_window_clean$Trial), 
                                       yes = 'Inanimate', 
                                       no  = 'Animate') )
```

## An Initial Look at Our Data: Descriptives and Visualization

Here, we'll inspect the descriptive statistics for our data, and make some initial visualizations.

### Aggregating Across Trial:

We perform a quick sanity check on our data, getting a glance at each subject's looking to the `Animate` for each condition.

```{r, warning=FALSE}
(data_summary <- describe_data(response_window_clean, data_options, 
                               describe_column='Animate', group_columns=c('ParticipantName','Target')))
```

We can also visualize our results. To do this, we use eyetrackingR's `time_window_data` type. 

```{r, warning=FALSE}
# aggregate by subject across the response window
response_window_agg_by_sub <- make_time_window_data(response_window_clean, 
                                             data_options, 
                                             aois='Animate',
                                             predictor_columns=c('Target','Age','MCDI_Total'),
                                             summarize_by = "ParticipantName")

# take a quick peek at data
plot(response_window_agg_by_sub, predictor_columns="Target")

# show condition means
describe_data(response_window_agg_by_sub, describe_column = "Prop", group_columns = "Target")
```

### Time-Course

Here, we'll visualize the time-course of looking. To do this, we use eyetrackingR's `time_sequence_data` type. This bins our data into time-bins of our choosing. Here, we use 100ms time-bins. 

```{r, warning=FALSE}
# aggregate across trials within subjects in time analysis
response_time <- make_time_sequence_data(response_window_clean, data_options, time_bin_size = 100, 
                                 predictor_columns = c("Target"),
                                 aois = "Animate"
                            )

# visualize time results
plot(response_time, predictor_column = "Target") + 
  theme_light() +
  coord_cartesian(ylim = c(0,1))
```

## Analyses

Our data is ready for some analyses.

### Windowed Data

#### Simple paired t-test

The simplest test we can do aggregates across the entire time-window, and compares the proportion-looking across conditions using a simple t-test on participants' means. We use the 'arcsin square root' transformation on proportion-looking.

```{r, warning=FALSE}
# simple t-test between conditions
t.test(ArcSin ~ Target, data= response_window_agg_by_sub, paired=TRUE)
```

#### Mixed-effects model

A more rigorous approach is *not* to summarize within each participant, but instead keep the trial-by-trial information and fit a linear mixed-effects model using `lmer`. This predicts infants' looking to the `Animate` AOI based on the `Target` condition of each trial while accounting for random intercept and slope across `Trial` (i.e., items) and `ParticipantName` (i.e., subjects). Here, we use the 'empirical logit' transformation on proportion-looking (Barr, 2007).

```{r, warning=FALSE}
response_window_agg <- make_time_window_data(response_window_clean, 
                                             data_options, 
                                             aois='Animate', 
                                             predictor_columns=c('Target','Age','MCDI_Total'))

# you should almost always center your predictors when performing regression analyses
response_window_agg$TargetC <- ifelse(response_window_agg$Target == 'Animate', .5, -.5)
response_window_agg$TargetC <- scale(response_window_agg$TargetC, center=TRUE, scale=FALSE)

# mixed-effects linear model on subject*trial data
model_time_window <- lmer(Elog ~ TargetC + (1 + TargetC | Trial) + (1 | ParticipantName), 
              data = response_window_agg, REML = FALSE)
broom::tidy(model_time_window, effects="fixed")
drop1(model_time_window,~.,test="Chi")
```

### Time-Series Data

#### Growth curve analysis

Growth-curve analysis lets us model the timecourse of attention by fitting curves to proportion-looking over the course of the trial, and statistically assessing the bends in these curves. 

EyetrackingR sets us up nicely for a growth-curve-analysis. Above, we used eyetrackingR to create `time_sequence_data`, called `response_time`. This creates data that is not only summarized into time-bins, but also includes "orthogonal polynomial timecodes," needed for a growth curve analysis.

```{r, warning=FALSE}
# again, center our predictor:
response_time$TargetC <- ifelse(response_time$Target == 'Animate', .5, -.5)
response_time$TargetC <- scale(response_time$TargetC, center=TRUE, scale=FALSE)

# Construct model
model_time_sequence <- lmer(Elog ~ TargetC*(ot1 + ot2 + ot3 + ot4 + ot5) + (1 | Trial) + (1 | ParticipantName), 
              data = response_time, REML = FALSE)
broom::tidy(model_time_sequence, effects = "fixed")
drop1(model_time_sequence, test="Chi")
```

Growth-curve models might be hard to understand at first. Fortunately, eyetrackingR lets us overlay the results of this model onto our graph of the data. The dark lines represent the model predictions.

```{r}
plot(response_time, predictor_column = "Target", dv = "Elog", model = model_time_sequence) +
  theme_light()
```

## Onset-contingent analysis

Another way of looking at our data is asking how participants switch between AOIs. We can compare the time it took for infants to switch to a new AOI depending on:

* whether they began the trial looking at the Target or not and,
* which type of trial they were participating in (i.e., whether the target was the Animate or the Inanimate)

### Visualize Proportion Switching Over the Trial

Here, we create an 'onset-contingent' plot (e.g., Fernald, 1998). The size of the gap between the solid line (switching from the non-Target) and the dashed line (switching from the Target) documents infants' performance in identifying the target: Infants should switch more quickly and more frequently *to* the Target than *from* it (but they also may be hesitant to *ever* shift from their preferred, animate targets...).

```{r, warning=FALSE}
# recode AOIs to target & distractor
response_window_clean$TrialTarget <- ifelse(test = response_window_clean$Target == 'Animate', 
                                            yes = response_window_clean$Animate, 
                                            no = response_window_clean$Inanimate)
response_window_clean$TrialDistractor <- ifelse(test = response_window_clean$Target == 'Animate', 
                                                yes = response_window_clean$Inanimate, 
                                                no = response_window_clean$Animate)
onsets <- make_onset_data(response_window_clean, data_options, onset_time = 15500, 
                          fixation_window_length = 100, target_aoi='TrialTarget')
# participants' ability to orient to the trial target overall:
plot(onsets) + theme(legend.text=element_text(size=5))
# participants' ability to orient to the trial target, split by which target:
plot(onsets, predictor_columns = "Target") + theme(legend.text=element_text(size=5))
# we can also visualize numeric predictors:
plot(onsets, predictor_columns = "MCDI_Total") + theme(legend.text=element_text(size=5))
```

### Test Time-To-Switch:

Here, we test the hypothesis that infants switch more readily to the target than away from it, using a mixed-effects model on time-to-switch.

```{r, warning= FALSE}
onset_switches <- make_switch_data(onsets, predictor_columns = "Target")

# visualize subject's switch times
plot(onset_switches, predictor_columns = c("Target"))

# center predictor:
onset_switches$FirstAOIC <- ifelse(onset_switches$FirstAOI == 'TrialTarget', .5, -.5)
onset_switches$FirstAOIC <- scale(onset_switches$FirstAOIC, center=TRUE, scale=FALSE) 
onset_switches$TargetC <- ifelse(onset_switches$Target == 'Animate', .5, -.5)
onset_switches$TargetC <- scale(onset_switches$TargetC, center=TRUE, scale=FALSE) 

# build model:
model_switches <- lmer(FirstSwitch ~ FirstAOIC*TargetC + 
                (1 | Trial) + (1 | ParticipantName), data=onset_switches, REML=FALSE)
broom::tidy(model_switches, effects="fixed")
drop1(model_switches,~.,test="Chi")
```

## Testing Time-Regions

Analyses that aggregate over the trial window tell us *whether* an effect was significant, and growth-curve analyses tell us the trajectory of our effect over the course of the trial. Often, however, we want to know *when* in a trial an effect is reliable. What is the onset, and how long does the effect last? EyetrackingR includes two types of analyses for answering these questions

### Bootstrapped splines analysis

This is novel non-parametric approach to time-course data. It involves

1. Fitting a smoothing curve to the data
2. Bootstrap-resampling these fits to build a distribution
3. Obtaining the 95% confidence intervals of this distribution in order to determine which time-bins differ significantly.

This is a useful technique for estimating the timepoints of divergence between two conditions, while the smoothing helps remove minor deviations. This can be especially helpful in infant data, which can be extremely noisy (note that this is not a replacement for techniques that control for false alarm rate, such a Bonferroni correction).

This method returns a list of divergences between your two conditions based on time windows in which the 95% confidence intervals did not include 0 (i.e., *p* < .05).

```{r, warning=FALSE}
bootstrapped_familiar <- make_boot_splines_data(response_time, 
                                              predictor_column = 'Target', 
                                              within_subj = TRUE, 
                                              samples = 1000, 
                                              alpha = .05,
                                              smoother = "smooth.spline") 

plot(bootstrapped_familiar)
bootstrap_analysis_familiar <- analyze_boot_splines(bootstrapped_familiar)
summary(bootstrap_analysis_familiar)
```

This analysis suggests that the effect is significant for virtually the entire time-window, starting as early as 15900ms.

### Bootstrapped cluster analysis

Here, we perform a different type of bootstrapping analyses, referred to as a cluster-based permutation analysis (Maris & Oostenveld, 2007). This analysis takes a summed statistic for each cluster of time bins that pass some level of significance, and compares each to the "null" distribution of sum statistics (obtained by bootstrap resampling data within the largest of the clusters).

This type of analysis should often give similar results to the above, but it has two main advanatges. First, it naturally controls the false-alarm rate while sacrificing little sensitivity. Second, the implementation in eyetrackingR allows you to use this method with a variety of statistical techniques (`t.test`, `wilcox.test`, `lm`, and `lmer`), so that continuous predictors, covariates, etc. can be included in the model being tested.

```{r, warning=FALSE}
response_time_by_subj = make_time_sequence_data(data= response_window_clean, 
                        data_options = data_options, 
                        time_bin_size = 100, 
                        aois= "Animate", 
                        predictor_columns = "Target",
                        summarize_by = "ParticipantName")
alpha = .05
num_sub = length(unique((response_window_clean$ParticipantName)))
threshold_t = qt(p = 1 - alpha/2, 
                 df = num_sub-1) # pick threshold t based on alpha = .05 two tailed
df_timeclust = make_time_cluster_data(response_time_by_subj, 
                                      test= "t.test", paired = TRUE,
                                      predictor_column = "Target", 
                                      threshold = threshold_t) 
plot(df_timeclust) +
  ylab("T-Statistic")
summary(df_timeclust)

clust_analysis = analyze_time_clusters(df_timeclust, within_subj = TRUE, paired=TRUE, samples=250)
clust_analysis

plot(clust_analysis) 
```





