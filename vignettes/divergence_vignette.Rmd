---
title: "Estimating time windows of divergence using eyetrackingR"
author: "Jacob Dink & Brock Ferguson"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Estimating time windows of divergence}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

> **Our Experiment**: Each eyetrackingR vignette uses the *eyetrackingR* package to analyze real data from a simple 2-alternative forced choice (2AFC) word recognition task administered to 19- and 24-month-olds.
> On each trial, infants were shown a picture of an animate object (e.g., a horse) and an inanimate object (e.g., a spoon). After inspecting the images, they disappeared and they heard a label referring to one of them (e.g., "The horse is nearby!"). Finally, the objects re-appeared on the screen and they were prompted to look at the target (e.g., "Look at the horse!").

## Overview of this vignette

In this vignette, we want to ascertain when a predictor had a significant effect during a trial. Analyses that aggregate over the trial window tell us *whether* an effect was significant, growth curve analyses tell us the trajectory of our effect over the course of the trial, and onset-contingent analyses can tell you reaction times for certain experimental designs. But none of these approaches allow you to ask: What is the onset of some predictor's effect, and how long does the effect last? eyetrackingR includes two types of analyses for answering these questions, both of which we cover here.

## Prerequisites

Before performing this analysis, we'll need to prepare and clean our dataset. Here we will to do this quickly and with few notes but, for more information, see the vignette on [preparing your data](preparing_your_data_vignette.html).

```{r results="hide"}
set.seed(42)

library("Matrix")
library("lme4")
library("ggplot2")
library("eyetrackingR")

data("word_recognition")
data <- make_eyetrackingr_data(word_recognition, 
                       participant_column = "ParticipantName",
                       trial_column = "Trial",
                       time_column = "TimeFromTrialOnset",
                       trackloss_column = "TrackLoss",
                       aoi_columns = c('Animate','Inanimate'),
                       treat_non_aoi_looks_as_missing = TRUE
                )

# subset to response window post word-onset
response_window <- subset_by_window(data, 
                                    window_start_time = 15500, 
                                    window_end_time = 21000, 
                                    rezero = FALSE)

# analyze amount of trackloss by subjects and trials
(trackloss <- trackloss_analysis(data = response_window))

# remove trials with > 25% of trackloss
response_window_clean <- clean_by_trackloss(data = response_window,
                                            trial_prop_thresh = .25)

# create Target condition column
response_window_clean$Target <- as.factor( ifelse(test = grepl('(Spoon|Bottle)', response_window_clean$Trial), 
                                       yes = 'Inanimate', 
                                       no  = 'Animate') )
```

To begin, we need to use `make_time_sequence_data` to generate a time-binned dataframe. We'll summarize by subjects for now.

```{r, warning=FALSE}
response_time <- make_time_sequence_data(response_window_clean,
                                  time_bin_size = 100, 
                                  predictor_columns = c("Target"),
                                  aois = "Animate",
                                  summarize_by = "ParticipantName"
                            )

# visualize timecourse
plot(response_time, predictor_column = "Target") + 
  theme_light() +
  coord_cartesian(ylim = c(0,1))
```

## Testing Time-Bins Independently

One straightforward method of testing for divergences is simply to perform a statistical test on each time-bin separately. This method is problematic, but walking through it and explaining why will help set up other methods.

EyetrackingR provides `analyze_time_bins`, a function that makes sequential tests like this easy to implement.

```{r, warning=FALSE}
tb_analysis <- analyze_time_bins(data = response_time, predictor_column = "Target", test = "t.test", alpha = .05)
plot(tb_analysis, type = "estimate")
summary(tb_analysis)
```

This method suggests that looking diverges across conditions as early as 16100. However, this method does not control the family-wise error rate. Because we are performing so many tests, we are bound to get *some* statistically significant results, even if no effect is actually present. If there is no real effect, if each test is completely independent, and if our alpha for each test is .05, then the odds of at least one false alarm is...

```{r}
alpha <- .05
num_time_bins <- nrow(tb_analysis)
(prob_no_false_alarm_per_bin <- 1-alpha)
(prob_no_false_alarm_any_bin <- prob_no_false_alarm_per_bin^num_time_bins)
(prob_at_least_one_false_alarm <- 1-prob_no_false_alarm_any_bin)
```

...almost 95%! Of course, two of the assumptions I described above aren't met: the independence assumption isn't met, and the "no real effect" assumption probably isn't met (given the analyses in the other vignettes). But we still need to control for family-wise error rate.

One approach is a bonferroni correction. If we simply lower our alpha according to the number of time-bins, then the family-wise error rate goes back down:

```{r}
alpha <- .05 / num_time_bins
(prob_no_false_alarm_per_bin <- 1-alpha)
(prob_no_false_alarm_any_bin <- prob_no_false_alarm_per_bin^num_time_bins)
(prob_at_least_one_false_alarm <- 1-prob_no_false_alarm_any_bin)
```

We apply this correction by using the `p_adjust_method`:

```{r, warning=FALSE}
tb_analysis_bonf <- analyze_time_bins(data = response_time, predictor_column = "Target", test = "t.test", alpha = .05,
                                 p_adjust_method = "bonferroni")
plot(tb_analysis_bonf)
summary(tb_analysis_bonf)
```

This method seems overly conservative, and indeed it is. This correction assumes the 'worst case scenario' that all time-bins are fully independent, which as we mentioned, is clearly not the case. Other methods are less stringent. Any method available in R's `p.adjust` function are available to `analyze_time_bins`. See the documentation of that function for more details. As mentioned there, there isn't really a good reason to use the Bonferroni method, because Holm's method controls family-wise error just as well, but sometimes is more powerful:

```{r, warning=FALSE}
tb_analysis_holm <- analyze_time_bins(data = response_time, predictor_column = "Target", test = "t.test", alpha = .05,
                                 p_adjust_method = "holm")
plot(tb_analysis_holm)
summary(tb_analysis_holm)
```

## Bootstrapped smoothed divergence analysis

One concern with multiple-testing with corrections is that it severely limits our power: by controlling the family-wise error rate, we sacrifice our ability to detect effects when they are present. 

In fact, even the uncorrected test seems to have made a small failure: it splines our runs of statistically significant time-bins in two, due to a small downward blip in one of the time-bins. It seems unlikely the effect actually vanished for this single time bin: instead, it seems more likely that eye-tracking data is noisy, and that we should try to ignore the small variations that originate from this noise.

One approach is to perform a statistical test that operates over a smoothed version of our data (similar to Wendt et al., 2014). This involves:

1. Resampling our with replacement from our data (e.g., randomly resampling subjects from each condition).
2. For each resample, fitting a smoothing curve to the data (choosing either `smooth.spline()`, `loess()`, or no smoother)
3. Obtaining the 95% confidence intervals of this distribution in order to determine which time-bins differ significantly.

This is a useful technique for estimating the timepoints of divergence between two conditions, while the smoothing helps remove minor deviations that might disrupt what would otherwise be considered a single divergent period. This can be especially helpful in infant data, which can be extremely noisy. Note that this approach can only deal with testing differences across two levels of a predictor (e.g., an experimental manipulation, not a continous covariate).

This method returns a list of divergences between your two conditions based on time windows in which (by default) the 95% confidence intervals did not include 0 (i.e., *p* < .05).

```{r, warning=FALSE}
tb_bootstrap <- analyze_time_bins(response_time, predictor_column = 'Target', within_subj = TRUE, test= 'boot_splines', alpha = .05)
plot(tb_bootstrap)
summary(tb_bootstrap)
```

We can see that this method (probably correctly) identified that our trial involves a single divergence in looking across conditions, rather than two divergences separated by a single time-bin.

However, it is important to note that this method doesn't explicitly control the family-wise error rate. And unfortunately, because this test doesn't produce a p-value for each bin, we can only perform a (manual) Bonferroni-correction.

```{r, warning=FALSE}
tb_bootstrap_bonf <- analyze_time_bins(response_time, predictor_column = 'Target', within_subj = TRUE, test= 'boot_splines', 
                                  alpha = .05/num_time_bins)
plot(tb_bootstrap_bonf)
summary(tb_bootstrap_bonf)
```

Note that, for designs such as this one, it's often important to test the effect by collapsing across both subjects and items. We can repeat the boot-splines analysis, but this time collapsing within *items* instead of *subjects*. Ideally, both analyses should give very similar results.

We do this below. Note that we want to set `within_subj` to FALSE-- this argument now essentially means `within_items`, and this is not true of this dataset (each item was either animate or inanimate).

```{r, warning=FALSE}
response_time_item <- make_time_sequence_data(response_window_clean,
                                  time_bin_size = 100, 
                                  predictor_columns = c("Target"),
                                  aois = "Animate",
                                  summarize_by = "Trial") # <---"Trial" corresponds to both item and trial 
tb_bootstrap_item <- analyze_time_bins(response_time_item, predictor_column = 'Target', within_subj = FALSE, test= 'boot_splines', 
                                  alpha = .05/num_time_bins)
plot(tb_bootstrap_item)
```

## Bootstrapped cluster-based permutation analysis

Above we saw problems both with false-alarms and sensitivity. This is not a zero-sum game. One approach that offers the best compromise is referred to as a cluster-based permutation analysis (Maris & Oostenveld, 2007). 

Essentially, this is a two step procedure. First, we run a test on each time bin that quantifies the statistical significance of the effect at each time bin. This acts as a "first pass," and we group together into clusters all adjacent bins that get through this first pass. We then shuffle the data, performing this test-then-cluster on each iteration of the shuffled data. This shuffled data tells us what kinds of clusters we should expect if there were no effect (i.e., randomly scrambled data).

---
In more detail, what eyetrackingR does is:

1. Run a statistical test on each time-bin of your data. This can be any valid and appropriate statistic test that quantifies the probability of the effect: t-test, wilcox.test, linear models, etc.-- whatever is appropriate for your data.
2. Take the time-bins whose test passed the a threshold statistic (e.g., t > ~2), and group them by adjacency. We will call these time-clusters.
3. For each time-cluster, calculate the sum of the statistics for the time-bins inside it.
4. Take the data and randomly shuffle it.
5. Perform (1)-(3) on the shuffled dataset. Save the biggest sum-statistic.
6. Repeat steps (4) and (5) hundreds of times. This will lead to a distribution of summed-statistics, each representing the results of a statistical test on shuffled data. Intuitively, this distribution represents what kind of sum-statistics we would expect to get by chance, if no effect were present (i.e., if the data were just randomly shuffled).
7. Compare the cluster-statistics from step (3) to the distribution found in (6) to obtain a p-value. So, for example, imagine we get a distribution of sum-statistics, and 6.4% of the sum-statistics in this distribution are more extreme the sum-statistic of our cluster, then our *p*-value for this cluster is *p* = .064.
---

This analysis has two main advanatges over the ones reviewed so far:

1. It naturally controls the false-alarm rate while sacrificing little sensitivity. 
2. The implementation in eyetrackingR allows you to use this method with a variety of statistical techniques (`t.test`, `wilcox.test`, `(g)lm`, and `(g)lmer`), so that continuous predictors, covariates, etc. can also be included in the model being tested. We even provide (experimental) support for using boot-splines as the test performed at each time bin.

First, we need to set a threshold for our "first pass," for what we will consider a valid cluster. This can be a source of misconceptions. The _size_ of the initial threshold you set doesn't matter too much, but you should set it in a principled way (e.g., don't run the cluster analysis, examine the result, then decide you want to use a different threshold). But perhaps surprisingly, the test controls the family-wise error rate, even if we don't choose a threshold that corresponds to p = .05. This is because the threshold affects both the first pass and the shuffled data. 

Here, we'll use a t.test for owe choose a slightly liberal threshold, to avoid splitting our clusters into two (like the t-test on each time)

To perform this procedure using eyetrackingR, we'll need to make a `time_sequence_data` dataframe. In this case, we already made above (`response_window`).

Next, we'll set the threshold for the t-statistic that will be considered a divergence. This can be a source of misconceptions. The _size_ of the initial threshold you set doesn't matter too much, but you should set it in a principled way (e.g., don't run the cluster analysis, examine the result, then decide you want to use a different threshold). Here, we'll just set it based on the t-distribution: ~2.06 corresponds to the usual statistic we would use in a t-test for this sample.

```{r, warning=FALSE}
num_sub = length(unique((response_window_clean$ParticipantName)))
threshold_t = qt(p = 1 - .05/2, 
                 df = num_sub-1) # pick threshold t based on alpha = .05 two tailed
levels(response_window_clean$Target) 
```

We can then look for initial clusters:

```{r, warning=FALSE}
stop("oo")
df_timeclust <- make_time_cluster_data(response_time, 
                                      test= "lmer",
                                      formula = Prop ~ Target + (1|Participant) + (1+Target|Trial),
                                      predictor_column = "Target", 
                                      threshold = threshold_t) 
plot(df_timeclust) +
  ylab("T-Statistic")
summary(df_timeclust)
```

The above tells us there are two potential clusters. As described in the procedure above, eyetrackingR next bootstraps a "null" distribution, which can be visualized:

```{r, warning=FALSE}
clust_analysis <- analyze_time_clusters(df_timeclust, within_subj = TRUE, paired=TRUE, 
                                        samples=100) # in practice, you should use a lot more
plot(clust_analysis)
```

How can we interpret these results? 

```{r, warning=FALSE}
summary(clust_analysis)
```

The probabilities listed above tell us the probability of seeing the effect of each cluster (or bigger) by chance. So we can report these as p-values with alpha=.05 (two-tailed). Each cluster that passes criterion is a cluster whose stretch of time exhibited a reliable effect.

## Summary

The two methods described here have several advantages and disadvantages. 

The boot-splines method is especially well-suited for simple designs with large but noisy effects. The splines help smooth over noise, and with such large effects, sensitivity-sacrificing methods that control for the false-alarm rate are less of a concern.

The time-cluster is better-suited for subtle but less noisy effects. This method is extremely sensitive, while setting an upper-bound on the family-wise false-alarm rate (% of spurious experiments, regardless of number-of-time-bins). This method also allows for a wider range of experimental designs and predictors that simply cannot be captured by the boot-splines method (e.g, continuous predictors, covariates, or anything else available to hierarchical models). The main disadvantage of this method is that it does not gracefully deal with noisy data. Because we simply group time-bins by adjacency, a single noisy time-bin (perhaps one with missing data) can break up a time-cluster that otherwise reliably exhibits an effect. In simple experimental designs with noisy data (many infant-studies), the boot-splines method can avoid this pitfall.

## References

Maris, E., Oostenveld, R., (2007). Nonparametric statistical testing of EEG- and MEG-data. Journal of Neuroscience Methods 164 (1), 177–190.

Wendt, D., Brand, T., & Kollmeier, B. (2014). An Eye-Tracking Paradigm for Analyzing the Processing Time of Sentences with Different Linguistic Complexities. PLoS ONE, 9(6), e100186. http://doi.org/10.1371/journal.pone.0100186.t003