---
title: "Confidence Interval for Divergence"
author: "Jacob Dink"
date: "October 7, 2015"
output: html_document
---

## Confidence Intervals for Timepoint of Looking Divergence

Traditional analyses can tell us whether a time window is significant, and can help us understand the trajectory of looking. However, there aren't any analyses that help us get a statistical estimate and confidence interval around *when* looking diverges (relative to baseline, or across two conditions). Below is some experimenting with a novel idea for estimating the timepoint at which looking diverges.

### Load our data in, etc.

```{r, warning=FALSE}
library("eyetrackingR")
library("dplyr")
data("word_recognition")

# set data options
data_options = set_data_options( 
  participant_column = "ParticipantName",
  trial_column = "Trial",
  time_column = "TimeFromTrialOnset",
  trackloss_column = "TrackLoss",
  aoi_columns = c('Animate','Inanimate')
)
data <- verify_dataset(word_recognition, data_options)
response_window <- subset_by_window(data, data_options, 
                                    window_start_time = 15500, 
                                    window_end_time = 21000, 
                                    rezero = FALSE)
response_window <- convert_non_aoi_to_trackloss(response_window, data_options)
response_window_clean <- clean_by_trackloss(data = response_window,
                                            data_options = data_options, 
                                            trial_prop_thresh = .25)
response_window_clean$Target <- as.factor( ifelse(test = grepl('(Spoon|Bottle)', response_window_clean$Trial), 
                                                  yes = 'Inanimate', 
                                                  no  = 'Animate') )
response_window_clean$TrialTarget <- with(response_window_clean, ifelse(Target == "Animate", Animate, Inanimate))
df_time = make_time_sequence_data(response_window_clean,data_options, time_bin_size = 100, aois = "Animate", predictor_columns = "Target")
```

### Difference Across Conditions

For each subject, take the mean difference between conditions for each timebin

```{r, warning=FALSE}
library("lme4")
df_time$Predict = 0
## try uncommenting these two lines when you've run through this whole vignette once:
# fit = lmer(data =df_time, formula = Prop ~ Target*(ot1+ot2+ot3+ot4) + (Target*(ot1+ot2+ot3+ot4) | ParticipantName ))
# df_time$Predict = predict(fit, df_time)
##
df_diff_pred = df_time %>%
  group_by(ParticipantName, Target, Time) %>%
  summarise(Predict = mean(Predict, na.rm=TRUE)) %>%
  tidyr::spread(Target, Predict) %>%
  mutate(PropDiff = Animate - Inanimate)

df_diff = df_time %>%
  group_by(ParticipantName, Target, Time) %>%
  summarise(Prop = mean(Prop, na.rm=TRUE)) %>%
  tidyr::spread(Target, Prop) %>%
  mutate(PropDiff = Animate - Inanimate)

library("ggplot2")
(g <- ggplot(df_diff, aes(x = Time, y = PropDiff)) +
  stat_summary(fun.y = mean, geom="line") +
  stat_summary(fun.y = mean, geom="line", data = df_diff_pred, linetype="dashed") +
  facet_wrap(~ ParticipantName)  +
  geom_hline(yintercept= 0, linetype="dotted") )
#df_diff$PropDiff <- df_diff_pred$PropDiff # uncomment to see below analyses with GCA smoothing
```


### Calculate Initial Divergence Based on a Rolling Window

We want to know when conditions first diverge. A bit sketchy to just assume any divergence counts (see e.g. ANCAT72).

No problem, let's just make a rolling window. For each timebin, calculate whether difference across conditions was above zero for not only that time bin, but *also its neighbors*. We only count it as a real divergence if its sticks around for enough timebins. Some experimenter degrees of freedom here (how big is the window?) but not so different than degrees of freedom for time-bin size.

Based on this smoothed measure, we pick the first point of divergence for each subject.

```{r, warning=FALSE}
width = 5 #  this time bin and his 4 neighbors (2 on each side)
df_diff_summary = df_diff %>%
  group_by(ParticipantName) %>%
  mutate(AllDiff = zoo::rollapply(PropDiff, function(x) all(x>0), width = width, fill=FALSE, align="left")) %>%
  summarise(FirstDiff = first(Time[which(AllDiff==TRUE)], order_by = Time, default=NA) )

num_subs_to_eventually_diverge = length(which(!is.na(df_diff_summary$FirstDiff)))
cat(num_subs_to_eventually_diverge, "out of", length(unique(df_diff_summary$ParticipantName)), "subjects eventually diverged.")

g + geom_vline(data = df_diff_summary, aes(xintercept=FirstDiff), linetype="dashed")
```

Seems like the procedure works fairly well. Only one too early false alarm? ANCAT90.

And now that we have a single number for each subject, we can get a confidence interval.

```{r, warning=FALSE}
distr = sapply(X = 1:5000, FUN = function(x) {
  mean(sample(df_diff_summary$FirstDiff, size = length(df_diff_summary$FirstDiff), replace = TRUE), na.rm=TRUE)
})
(conf = c(quantile(distr, probs = .025), Mean = mean(distr), quantile(distr, probs = .975)))

plot(df_time, predictor_column="Target") +
  geom_vline(xintercept = conf, linetype = c(3,2,3), alpha = .8)
```

This might seem like a surprisingly late estimate, relative to what we're seeing in the graph. It seems to be due to the skew in the data: the majority of the subjects diverge early, but the ones who diverge late can diverge very late:

```{r, warning=FALSE}
quickplot(x= df_diff_summary$FirstDiff, xlim = c(15500, 21000))

plot(df_time, predictor_column="Target") +
  geom_vline(xintercept = df_diff_summary$FirstDiff, alpha = .5) +
  xlab("Time (each line = a subject's first divergence)")
```

This makes sense, based on the nature of the data. We suspect that the true divergence point is around 15500-17000. We have _a priori_ knowledge about how early it could possibly be: We know it *cannot* be before 15500, so we subset the data to rule out that possibility. However, we don't have any _a priori_ knowledge about how *late* the divergence could possibly be. So our distribution will inherently be skewed.

On the one hand, this skew could be due to the fact that we've allowed false-positives on the late end, while disallowing them on the early end. But allowing for early FPs seems bad: (a) we're throwing out information (our knowledge that meaningful differences won't exist there), (b) playing with that on this dataset yields a lot of spurious/early diverges, for a way too early estimate. This shouldn't be surprising, because there are hardly any checks in place to verify a divergence is real within a subject: it primarily depends on a good _a priori_ window (set in a principled way via cluster_analysis). 

On the other hand, this skew could simply be a property of divergence-as-response-to-stimuli: you can't diverge early, but you can diverge late. 

The problem seems to ultimately stem from poor estimates: we are estimating each subject's divergence point and this is simply too noisy.

A better solution is a model based approach. Since we should already be doing a GCA on our data, we should use the information this model gives us to estimate each subject's divergence. This allows us to use the overall data to make better inferences for the trajectory of each individual subject, which in turn lets us better estimate each one's divergence point.

To check this out, re-run the above code after uncommenting the two lines in the second code chunk. This gives us a much less skewed result, and therefore our estimate for the divergence point does a much better job lining up with what we see in the aggregated data.


#### Some More Thoughts:

* What should be done for subjects who _never_ diverge in looking? Could do something like give them a first-diff time corresponding to the end of the trial. But I think it might be better just to exclude them in first-diff time calculations, and be explicit about what's being calculated. E.g., in a results section "of 40 subjects, 38 showed heightened looking to X at some point in the trial in condition A compared to condition B. Of these 38, the average time at which this preference emerged was 500ms(+/- 84ms)."
* This method could also be used for looking at a single AOI, regardless of condition/study-design. When is looking at AOI X reliably above some threshold? This analysis is probably *only* appropriate if its guaranteed at start time that they aren't looking at the AOI. Otherwise the estimate is biased, because some subjects will start by chance on the AOI. Have to think about this more.
