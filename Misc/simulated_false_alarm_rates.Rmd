---
title: "False Alarm Rate Simulations"
author: "Jacob Dink"
date: "November 12, 2015"
output: 
  html_document: 
    fig_height: 4
    fig_width: 6
---

### Prepare Data:

```{r, warning=FALSE, echo=FALSE}
library("Matrix")
library("lme4")
library("ggplot2")
library("eyetrackingR")
suppressPackageStartupMessages(library("dplyr"))

look_at_aoi <- function(ppt_offset, item_offset, orig_dat, time_in_trial) {
  bump_time_start <- sample(x = time_in_trial[-length(time_in_trial)], size = 1)
  bump_time_end <- sample(x = time_in_trial[time_in_trial>bump_time_start], size = 1)
  bump_offset <- rnorm(n = length(time_in_trial), mean = as.numeric(time_in_trial>bump_time_start & time_in_trial<bump_time_end))
  time_in_trial <-time_in_trial - min(time_in_trial)
  time_in_trial <-time_in_trial / max(time_in_trial)
  time_offset <- rnorm(n = length(time_in_trial), mean = -time_in_trial)
  baseline_odds <- rnorm(length(orig_dat))
  prob_vec <- boot::inv.logit(baseline_odds + ppt_offset + item_offset + time_offset + bump_offset)
  rbinom(n = length(prob_vec), size = 1, prob = prob_vec)
}

data("word_recognition")
data <- make_eyetrackingr_data(word_recognition, 
                               participant_column = "ParticipantName",
                               trial_column = "Trial",
                               time_column = "TimeFromTrialOnset",
                               trackloss_column = "TrackLoss",
                               aoi_columns = c('Animate','Inanimate'),
                               treat_non_aoi_looks_as_missing = TRUE
)

# subset to response window post word-onset
response_window <- subset_by_window(data, 
                                    window_start_time = 15500, 
                                    window_end_time = 21000, 
                                    rezero = FALSE)

# analyze amount of trackloss by subjects and trials
trackloss <- trackloss_analysis(data = response_window)

# remove trials with > 25% of trackloss
response_window_clean <- clean_by_trackloss(data = response_window, trial_prop_thresh = .25)

# create Target condition column
response_window_clean$Target <- as.factor( ifelse(test = grepl('(Spoon|Bottle)', response_window_clean$Trial), 
                                                  yes = 'Inanimate', 
                                                  no  = 'Animate') )

```

## Test 1: Boot-splines and Item-Random-Effects

### Generate Fake Data, Run Tests, Repeat:

```{r, warning=FALSE}
test_cluster <- TRUE
sigma_item_offset <- 1/2 # try values from 1/100 to 2
res_overall <- list()
res_bs <- list()
res_clust <- list()
set.seed(41)
for (i in 1:20) {
  # Make Data:
  fake_data <- response_window_clean %>%
    group_by(ParticipantName) %>%
    mutate(ParticipantOffset = rnorm(1, sd=1)) %>%
    group_by(Trial) %>%
    mutate(ItemOffset = rnorm(1, sd = sigma_item_offset)) %>%
    group_by(ParticipantName, Trial) %>%
    mutate(Animate = look_at_aoi(ppt_offset = ParticipantOffset, item_offset = ItemOffset, orig_dat = Animate, time_in_trial = TimeFromTrialOnset),
           Inanimate = !Animate)
  
  # Overall Effect:
  tw_dat <- make_time_window_data(fake_data, aois = "Animate", predictor_columns = "Target")
  fit <- lmer(formula= Prop ~ Target + 
                (1|Trial) +              # <---------- try commenting this out
                (1|ParticipantName), 
              data = tw_dat)
  res_overall[[i]] <- broom::tidy(fit, effects="fixed")
  
  # Bootsplines:
  ts_dat_long <- make_time_sequence_data(fake_data, time_bin_size = 100, 
                                    aois = "Animate", predictor_columns = "Target")
  ts_dat <- make_time_sequence_data(fake_data, time_bin_size = 100, 
                                    aois = "Animate", predictor_columns = "Target", summarize_by = "ParticipantName")
  bs_dat <- make_boot_splines_data(ts_dat, predictor_column = "Target", within_subj = TRUE,
                                   #alpha = .05/55, #bonferonni correction doesn't help much
                                   samples = 500)
  bs_anal <- analyze_boot_splines(bs_dat)
  res_bs[[i]] <- list(
    prop_fa = length(which(bs_anal$Significant)) / length(bs_anal$Significant), 
    mean_diff = mean(ifelse(bs_anal$Significant, bs_anal$MeanDiff, NA), na.rm=TRUE)
  )
  
  # Cluster
  if (test_cluster) {
    effect_dir <- sign(res_overall[[i]]$estimate[2])
    cl_dat <- make_time_cluster_data(ts_dat_long, predictor_column = "Target", test = "lmer", 
                           threshold = effect_dir * 2.06, 
                           formula = Prop ~ Target + (1|ParticipantName) + (1|Trial))
    if (nrow(get_time_clusters(cl_dat))>1) {
      cl_anal <- analyze_time_clusters(cl_dat, within_subj = TRUE, samples = 500, parallel = TRUE)
      clusts <- get_time_clusters(cl_anal)
      res_clust[[i]] <- list(
        prop_fa = sum( with(clusts,NumBins[Prob < .025]) ) / nrow(cl_anal$time_bin_summary)
      )
    } else {
      res_clust[[i]] <- list(
        prop_fa = 0
      )
    }
  }
}
```

### Evaluate Results:

#### LMER on Overall Data:

```{r}
res_overall <- bind_rows(lapply(res_overall, function(x) x[2,]))
plot(res_overall$statistic)
mean(abs(res_overall$statistic)>2.06) # perfect
```

LMER seems to control false-alarm rate, because we added a random-intercept for trial. If we remove this random-intercept (see comment in code above), then the false-alarm rate goes way up.

#### Boot-Splines Approach:

Here's the false-alarm rate for boot-splines:

```{r}
bs_fa <- sapply(res_bs, function(x) x$prop_fa)
mean(bs_fa) # false-alarm rate for boot-splines
```

And here's the average estimated size of the effect for bins where it was judged significant:

```{r}
bs_mean_diff <- sapply(res_bs, function(x) ifelse(is.nan(x$mean_diff), 0, x$mean_diff))
plot(bs_mean_diff); abline(h=0)
```

Here's an example:
```{r}
plot(ts_dat, predictor_column = "Target")
plot(bs_anal) # an example
```

Boot-splines can't control the false-alarm rate when items potentially vary a lot (bonferroni alone can't fix this). Random variation in the items creates what looks like systematic patterns in the looking-times when you ignore those items.

The obvious fix here is to run both by-participants and by-items boot-splines. You can try this by setting `within_subj=FALSE,` and `summarize_by = 'Trial'`. This alone doesn't get the false-alarm rate down; however, if you *also* do bonferoni correction, then the false-alarm rate is (finally) controlled.

## Test 2: Cluster Analysis and Missing Data

Let's test out cluster-analysis on its weakness: noisy data. First, here's baseline:

```{r}
df_time <- make_time_sequence_data(data = response_window_clean, time_bin_size = 100, aois = "Animate", 
                                   predictor_columns = "Target", summarize_by = "ParticipantName")

df_tb <- analyze_time_bins(df_time, predictor_column = "Target", test = "t.test", threshold = 2.06, 
                                   paired=TRUE)
plot(df_tb)
```

We can see that there is a nice big chunk of significant difference from 16000-19000, and then a second smaller chunk that seems to be significant too.

What I'll do now is make a ton of the data missing around Time = 18000 to see if I can split this chunk into two smaller chunks.

```{r}
set.seed(17)
response_window_md <- response_window_clean %>% # md = missing data
  group_by(ParticipantName, Trial) %>%
  mutate(LostTrial = rbinom(n = 1, size = 1, prob = .75)) %>%
  ungroup()
response_window_md$Animate <- with(response_window_md, 
                                ifelse(test = LostTrial & (TimeFromTrialOnset > 17700 & TimeFromTrialOnset < 18200), 
                                       yes = NA, no = Animate))
response_window_md$Inanimate <- !response_window_md$Animate
df_time_md <- make_time_sequence_data(data = response_window_md, time_bin_size = 100, aois = "Animate", 
                                   predictor_columns = "Target", summarize_by = "ParticipantName")
plot(df_time, predictor_column = "Target") + coord_cartesian(ylim=c(0,1)) + ggtitle("original data")
plot(df_time_md, predictor_column = "Target") + coord_cartesian(ylim=c(0,1)) + ggtitle("w/missing")
```

When we plot the parameter estimates over time, we see that the effect is more variable in this stretch of time, but not any less strong:

```{r}
df_tb_md <- analyze_time_bins(df_time_md, predictor_column = "Target", test = "t.test", threshold = 2.1, 
                                   paired=TRUE)
plot(df_tb, type = "estimate") + coord_cartesian(ylim = c(0,.75))
plot(df_tb_md, type = "estimate") + coord_cartesian(ylim = c(0,.75))
```

But when plot the t-statistic over time, we see a sharp drop during this missing-data time period, one that makes the clusters act poorly.

```{r, warning=FALSE}
df_clust_md <- make_time_cluster_data(df_time_md, predictor_column = "Target", test = "t.test", threshold = 2.1, 
                                   paired=TRUE)
plot(df_clust_md)
summary(df_clust_md)
```

In this case, the effect is so massive that it's reliable in the two other main clusters still. Still, we have to make this weird (and fallacious) conclusion that our effect didn't occur in the middle of the trial.

```{r, warning=FALSE}
cluster_anal_md <- analyze_time_clusters(df_clust_md, within_subj = T, paired=T, samples = 200, parallel = T)
plot(cluster_anal_md)
summary(cluster_anal_md)
```

## Test 3: Boot-splines and sudden-shifts

Compare the above to boot-splines:

```{r}
bs_dat_md <- make_boot_splines_data(df_time_md, predictor_column = "Target", within_subj = T)
plot(bs_dat_md)
summary(analyze_boot_splines(bs_dat_md))
```

The missing/noisy data doesn't cause any problem for boot-splines whatsoever.

But what if, instead of getting the same parameter estimate during that period but with lower certainty, we actually got a lower parameter estimate with *high* certainty? Can the splining procedure capture this?

In other words, boot-splines should hopefully smooth away uncertainty, but maintain certain but abrupt shifts.

```{r}
set.seed(17)
response_window_abrupt <- response_window_clean %>%
  group_by(ParticipantName, Trial) %>%
  mutate(WeirdTrial = rbinom(n = 1, size = 1, prob = .5) & (Target=="Animate")) %>%
  ungroup()
response_window_abrupt$Animate <- with(response_window_abrupt, 
                                ifelse(test = WeirdTrial & (TimeFromTrialOnset > 17700 & TimeFromTrialOnset < 18200), 
                                       yes = FALSE, no = Animate)) 
  # if it's a "weird"" trial, they looked away from animate for this period
response_window_abrupt$Inanimate <- !response_window_abrupt$Animate
df_time_abrupt <- make_time_sequence_data(data = response_window_abrupt, time_bin_size = 100, aois = "Animate", 
                                   predictor_columns = "Target", summarize_by = "ParticipantName")
plot(df_time, predictor_column = "Target") + coord_cartesian(ylim=c(0,1)) + ggtitle("original data")
plot(df_time_abrupt, predictor_column = "Target") + coord_cartesian(ylim=c(0,1)) + ggtitle("w/abrupt shift")
```

```{r}
bs_dat_abrupt <- make_boot_splines_data(df_time_abrupt, predictor_column = "Target", within_subj = T)
plot(bs_dat_abrupt)
summary(analyze_boot_splines(bs_dat_abrupt))
```

Respect.

### Conclusion

Data with random-effect structure seems to be what gives boot-splines trouble, while cluster-analysis is robust to this (because it can just take an LMER model). However, boot-splines can be salvaged by running within-items and doing bonferonni correction.

Data with noise in some time-bins seems to be what gives cluster analysis trouble, while boot-splines is highly robust to this. I don't know how to salvage this, unfortunately.

**Brainstorming:**

* Smooth first? (problem: getting too far away from real data)
* Under-weight bins with low sample-size (problem: how?)
* Cluster based on param. estimate rather than test-statistic. (This last one is intriguing. I'm imagining doing something with the derivative of the param. estimate over time...)



