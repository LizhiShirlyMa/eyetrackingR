---
title: "Testing Properties of Divergence Analyses with Simulations"
author: "Jacob Dink"
date: "November 15, 2015"
output: html_document
---

### Notes/Brainstorming:

Tests for Initial Analysies:

 * Divergence halfway through Trial
    * FA %
    * FWER
    * Correct Discovery %
    * Estimated Time-of-Divergence (bias?)
    * Magnitude of divergence?
    
Test for Follow-up Analyses:

 * Conditioning high-FWER analyses on preliminary overall analyses
    (Brock: I'm not sure this makes sense. What's a false discovery when validated by lmer? I think we can recommend this without a simulation.)
    * Only run boot-splines when there's an overall effect across entire window
    * Only run boot-splines when there's an interaction term in the growth-curve
 * By-Items Analysis vs. Collapsing Across Items
    (Brock: interesting, but maybe not paper worthy. not sure if it's a big enough issue)
    * Cluster-Analysis does well
    * Splines does poorly, but easy to fix
 * Effects of time-bin with high amount of noise 
    * Splines do well
    * Cluster-analysis does poorly
 * Adjusting Threshold of Cluster-Analysis
    (Brock: our primary analysis tweaks the alpha in 2 different ways already so I think this is redundant.)
    * A prior
    * Post-hoc
 * Abrupt End-of-Divergence for Brief Period
    * Does splines falsely smooth this away?

Before we run the final analyses:
  * increase sim_factor to 50

```{r, echo=FALSE}
#devtools::install_github('jwdink/eyetrackingR')
library("eyetrackingR")
library("ggplot2")
suppressPackageStartupMessages(library("dplyr"))
library("pbapply")
suppressPackageStartupMessages(library("foreach"))
doMC::registerDoMC()

#set.seed(40)

tb_size <- 100
sim_factor <- 50 # of simulations for each Nsubjects/Nitems/pref cell

  # create vectors for permutations
  N_subjects = rep(c(10,30,50,70), each=sim_factor)
  N_items = c(10)
  pref = c(.60,.75,.90)

n_bootstrap_samples <- 250 # (for clusters and bootstraps)
effect_start <- 2500
trial_length <- 5000
num_time_bins <- trial_length / tb_size
fdr_correction <- trial_length / 250 # set based on independent saccade bins.... experimental

f <- function(d, i) mean(d[i], na.rm=TRUE)
```

## T.Tests

### No Correction:

```{r}
t_tests <- data.frame(
                  test = 't_tests',
                  correction_factor = 0,
                  effective_alpha = .05
              ) 

t_tests_results <- foreach(N_subjects=N_subjects, .combine=rbind) %:%
                   foreach(N_items=N_items, .combine=rbind) %:%
                   foreach(pref=pref, .combine=rbind) %dopar% {
  # begin analysis
  df <- simulate_eyetrackingr_data(num_participants = N_subjects, num_items_per_condition = N_items, pref = pref, pref_window = c(effect_start,trial_length))
  df_time_sub <- make_time_sequence_data(df, time_bin_size = tb_size, predictor_columns = "Condition", aois = "AOI1", summarize_by = "Participant")
  tb_anal <- analyze_time_bins(df_time_sub, predictor_column = "Condition", test = "t.test", alpha = .05, quiet=TRUE, formula = Elog ~ Condition)
  tb_anal$Sig <- with(tb_anal, !is.na(PositiveRuns|NegativeRuns))
  # end analysis
  
  # begin return data
  with(tb_anal,
       data.frame(
          N_subjects = N_subjects,
          N_items = N_items,
          pref = pref,
          avg_rt = mean(df$RT, na.rm=TRUE),
          rt_lower_quart = quantile(df$RT, probs = .25),
          rt_upper_quart = quantile(df$RT, probs = .75),
          false_alarm_bins = sum(Sig[Time <  effect_start-tb_size]),
          correct_discovery_bins = sum(Sig[Time >= effect_start]),
          first_discovery_time = Time[Time >= effect_start-tb_size & Sig][1],
          n_clusters = NA
        )
       )
  # end return data
}

# merge results into dataframe
t_tests <- cbind(t_tests, t_tests_results)
```

### Bonferoni correction:

```{r}
t_tests_bonf <- data.frame(
                  test = 't_tests_bonf',
                  correction_factor = num_time_bins,
                  effective_alpha = .05/num_time_bins
              )

t_tests_bonf_results <- foreach(N_subjects=N_subjects, .combine=rbind) %:%
                   foreach(N_items=N_items, .combine=rbind) %:%
                   foreach(pref=pref, .combine=rbind) %dopar% {

  # begin analysis
  df <- simulate_eyetrackingr_data(num_participants = N_subjects, num_items_per_condition = N_items, pref = pref, pref_window = c(effect_start,trial_length))
  df_time_sub <- make_time_sequence_data(df, time_bin_size = tb_size, predictor_columns = "Condition", aois = "AOI1", summarize_by = "Participant")
  tb_anal <- analyze_time_bins(df_time_sub, predictor_column = "Condition", test = "t.test", alpha = .05/num_time_bins, quiet=TRUE, formula = Elog ~ Condition)
  tb_anal$Sig <- with(tb_anal, !is.na(PositiveRuns|NegativeRuns))
  # end analysis
  
  # begin return data
  with(tb_anal,
       data.frame(
          N_subjects = N_subjects,
          N_items = N_items,
          pref = pref,
          avg_rt = mean(df$RT, na.rm=TRUE),
          rt_lower_quart = quantile(df$RT, probs = .25),
          rt_upper_quart = quantile(df$RT, probs = .75),
          false_alarm_bins = sum(Sig[Time <  effect_start-tb_size]),
          correct_discovery_bins = sum(Sig[Time >= effect_start]),
          first_discovery_time = Time[Time >= effect_start-tb_size & Sig][1],
          n_clusters = NA
        )
       )
  # end return data
  
}

# merge results into dataframe
t_tests_bonf <- cbind(t_tests_bonf,t_tests_bonf_results)
```

### FDR correction:

```{r}
t_tests_fdr <- data.frame(
                  test = 't_tests_fdr',
                  correction_factor = fdr_correction,
                  effective_alpha = .05/fdr_correction
              )

t_tests_fdr_results <- foreach(N_subjects=N_subjects, .combine=rbind) %:%
                   foreach(N_items=N_items, .combine=rbind) %:%
                   foreach(pref=pref, .combine=rbind) %dopar% {
  
  # begin analysis
  df <- simulate_eyetrackingr_data(num_participants = N_subjects, num_items_per_condition = N_items, pref = pref, pref_window = c(effect_start,trial_length))
  df_time_sub <- make_time_sequence_data(df, time_bin_size = tb_size, predictor_columns = "Condition", aois = "AOI1",  summarize_by = "Participant")
  tb_anal <- analyze_time_bins(df_time_sub, predictor_column = "Condition", test = "t.test", alpha = .05/fdr_correction, quiet=TRUE, formula = Elog ~ Condition)
  tb_anal$Sig <- with(tb_anal, !is.na(PositiveRuns|NegativeRuns))
  # end analysis
  
  # begin return data
  with(tb_anal,
       data.frame(
          N_subjects = N_subjects,
          N_items = N_items,
          pref = pref,
          avg_rt = mean(df$RT, na.rm=TRUE),
          rt_lower_quart = quantile(df$RT, probs = .25),
          rt_upper_quart = quantile(df$RT, probs = .75),
          false_alarm_bins = sum(Sig[Time <  effect_start-tb_size]),
          correct_discovery_bins = sum(Sig[Time >= effect_start]),
          first_discovery_time = Time[Time >= effect_start-tb_size & Sig][1],
          n_clusters = NA
        )
       )
  # end return data
  
}

# merge results into dataframe
t_tests_fdr <- cbind(t_tests_fdr, t_tests_fdr_results)
```

## Bootstrapped Smoothing Splines

### No Correction:

```{r}
boot_splines <- data.frame(
                  test = 'boot_splines',
                  correction_factor = 0,
                  effective_alpha = .05
              )

boot_splines_results <- foreach(N_subjects=N_subjects, .combine=rbind) %:%
                   foreach(N_items=N_items, .combine=rbind) %:%
                   foreach(pref=pref, .combine=rbind) %dopar% {
  
  # begin analysis
  df <- simulate_eyetrackingr_data(num_participants = N_subjects, num_items_per_condition = N_items, pref = pref, pref_window = c(effect_start,trial_length))
  df_time_sub <- make_time_sequence_data(df, time_bin_size = tb_size, predictor_columns = "Condition", aois = "AOI1", summarize_by = "Participant")
  bs_dat <- make_boot_splines_data(df_time_sub, predictor_column = "Condition", within_subj = FALSE, alpha = .05, samples = n_bootstrap_samples)
  bs_anal <- analyze_boot_splines(bs_dat)
  # end analysis
  
  # begin return data
  with(bs_anal,
       data.frame(
          N_subjects = N_subjects,
          N_items = N_items,
          pref = pref, 
          avg_rt = mean(df$RT, na.rm=TRUE),
          rt_lower_quart = quantile(df$RT, probs = .25),
          rt_upper_quart = quantile(df$RT, probs = .75),
          false_alarm_bins = sum(Significant[Time <  effect_start-tb_size]),
          correct_discovery_bins = sum(Significant[Time >= effect_start]),
          first_discovery_time = Time[Time >= effect_start-tb_size & Significant][1],
          n_clusters = length(rle(Significant)$values[which(rle(Significant)$values == TRUE)])
        )
       )
  # end return data
  
}

# merge results into dataframe
boot_splines <- cbind(boot_splines, boot_splines_results)
```

### Bonferoni correction:

```{r}
boot_splines_bonf <- data.frame(
                  test = 'boot_splines_bonf',
                  correction_factor = num_time_bins,
                  effective_alpha = .05/num_time_bins
              )

boot_splines_bonf_results <- foreach(N_subjects=N_subjects, .combine=rbind) %:%
                   foreach(N_items=N_items, .combine=rbind) %:%
                   foreach(pref=pref, .combine=rbind) %dopar% {
  
  # begin analysis
  df <- simulate_eyetrackingr_data(num_participants = N_subjects, num_items_per_condition = N_items, pref = pref, pref_window = c(effect_start,trial_length))
  df_time_sub <- make_time_sequence_data(df, time_bin_size = tb_size, predictor_columns = "Condition", aois = "AOI1", summarize_by = "Participant")
  bs_dat <- make_boot_splines_data(df_time_sub, predictor_column = "Condition", within_subj = FALSE, alpha = .05/num_time_bins, samples = n_bootstrap_samples)
  bs_anal <- analyze_boot_splines(bs_dat)
  # end analysis
  
  # begin return data
  with(bs_anal,
       data.frame(
          N_subjects = N_subjects,
          N_items = N_items,
          pref = pref, 
          avg_rt = mean(df$RT, na.rm=TRUE),
          rt_lower_quart = quantile(df$RT, probs = .25),
          rt_upper_quart = quantile(df$RT, probs = .75),
          false_alarm_bins = sum(Significant[Time <  effect_start-tb_size]),
          correct_discovery_bins = sum(Significant[Time >= effect_start]),
          first_discovery_time = Time[Time >= effect_start-tb_size & Significant][1],
          n_clusters = length(rle(Significant)$values[which(rle(Significant)$values == TRUE)])
        )
       )
  # end return data
  
}

# merge results into dataframe
boot_splines_bonf <- cbind(boot_splines_bonf, boot_splines_bonf_results)
```

### FDR correction:

```{r}
boot_splines_fdr <- data.frame(
                  test = 'boot_splines_fdr',
                  correction_factor = fdr_correction,
                  effective_alpha = .05/fdr_correction
              )

boot_splines_fdr_results <- foreach(N_subjects=N_subjects, .combine=rbind) %:%
                   foreach(N_items=N_items, .combine=rbind) %:%
                   foreach(pref=pref, .combine=rbind) %dopar% {
  
  # begin analysis
  df <- simulate_eyetrackingr_data(num_participants = N_subjects, num_items_per_condition = N_items, pref = pref, pref_window = c(effect_start,trial_length))
  df_time_sub <- make_time_sequence_data(df, time_bin_size = tb_size, predictor_columns = "Condition", aois = "AOI1", summarize_by = "Participant")
  bs_dat <- make_boot_splines_data(df_time_sub, predictor_column = "Condition", within_subj = FALSE, alpha = .05/fdr_correction, samples = n_bootstrap_samples)
  bs_anal <- analyze_boot_splines(bs_dat)
  # end analysis
  
  # begin return data
  with(bs_anal,
       data.frame(
          N_subjects = N_subjects,
          N_items = N_items,
          pref = pref, 
          avg_rt = mean(df$RT, na.rm=TRUE),
          rt_lower_quart = quantile(df$RT, probs = .25),
          rt_upper_quart = quantile(df$RT, probs = .75),
          false_alarm_bins = sum(Significant[Time <  effect_start-tb_size]),
          correct_discovery_bins = sum(Significant[Time >= effect_start]),
          first_discovery_time = Time[Time >= effect_start-tb_size & Significant][1],
          n_clusters = length(rle(Significant)$values[which(rle(Significant)$values == TRUE)])
        )
       )
  # end return data
  
}

# merge results into dataframe
boot_splines_fdr <- cbind(boot_splines_fdr, boot_splines_fdr_results)
```

## Clusters

### No Correction:

```{r}
clusters <- data.frame(
                  test = 'clusters',
                  correction_factor = 0,
                  effective_alpha = .05
              )

clusters_results <- foreach(N_subjects=N_subjects, .combine=rbind) %:%
                   foreach(N_items=N_items, .combine=rbind) %:%
                   foreach(pref=pref, .combine=rbind) %dopar% {
  
  # begin analysis
  df <- simulate_eyetrackingr_data(num_participants = N_subjects, num_items_per_condition = N_items, pref = pref, pref_window = c(effect_start,trial_length))
  df_time_sub <- make_time_sequence_data(df, time_bin_size = tb_size, predictor_columns = "Condition", aois = "AOI1", summarize_by = "Participant")
  
  alpha = .05
  num_sub = length(unique((df_time_sub$Participant)))
  threshold_t = qt(p = 1 - alpha/2, 
                   df = num_sub-1) # pick threshold t based on alpha = .05 two tailed
  
  cl_dat <- make_time_cluster_data(data = df_time_sub, predictor_column = "Condition", test = "t.test", threshold = threshold_t, quiet = TRUE, formula = Elog ~ Condition)
  if ( nrow(get_time_clusters(cl_dat))>0 ) {
    cl_anal <- analyze_time_clusters(cl_dat, within_subj = FALSE, samples = n_bootstrap_samples, parallel = TRUE)
    cl_clusts <- get_time_clusters(cl_anal)
    cl_clusts$NumBins <- with(cl_clusts, (EndTime - StartTime)/tb_size)
    cl_sig_clusts <- filter(cl_clusts, cl_clusts$Probability<alpha)
    out <- with(cl_sig_clusts, 
              data.frame(
                N_subjects = N_subjects,
                N_items = N_items,
                pref = pref, 
                avg_rt = mean(df$RT, na.rm=TRUE),
                rt_lower_quart = quantile(df$RT, probs = .25),
                rt_upper_quart = quantile(df$RT, probs = .75),
                false_alarm_bins = sum((EndTime[EndTime < (effect_start-tb_size) & StartTime < (effect_start-tb_size)] - StartTime[EndTime < (effect_start-tb_size) & StartTime < (effect_start-tb_size)])/tb_size, ((effect_start-tb_size)-StartTime[EndTime >= (effect_start-tb_size) & StartTime < (effect_start-tb_size)])/tb_size),
                correct_discovery_bins = sum((EndTime[StartTime >= (effect_start-tb_size)] - StartTime[StartTime >= (effect_start-tb_size)])/tb_size, (EndTime[StartTime < (effect_start-tb_size) & EndTime > (effect_start-tb_size)]-(effect_start-tb_size))/tb_size),
                first_discovery_time = StartTime[EndTime >= (effect_start-tb_size)][1],
                n_clusters = nrow(get_time_clusters(cl_dat))
              )  
        )
  } else {
    out <- data.frame(
                N_subjects = N_subjects,
                N_items = N_items,
                pref = pref, 
                avg_rt = mean(df$RT, na.rm=TRUE),
                rt_lower_quart = quantile(df$RT, probs = .25),
                rt_upper_quart = quantile(df$RT, probs = .75),
                false_alarm_bins = 0,
                correct_discovery_bins = 0,
                first_discovery_time = NA,
                n_clusters = 0
            )
  }
  # end analysis
  
  # begin return data
  return(out)
  # end return data
  
}

# merge results into dataframe
clusters <- cbind(clusters, clusters_results)
```

### Bonferoni correction:

```{r}
clusters_bonf <- data.frame(
                  test = 'clusters_bonf',
                  correction_factor = num_time_bins,
                  effective_alpha = .05/num_time_bins
              )

clusters_bonf_results <- foreach(N_subjects=N_subjects, .combine=rbind) %:%
                   foreach(N_items=N_items, .combine=rbind) %:%
                   foreach(pref=pref, .combine=rbind) %dopar% {
  
  # begin analysis
  df <- simulate_eyetrackingr_data(num_participants = N_subjects, num_items_per_condition = N_items, pref = pref, pref_window = c(effect_start,trial_length))
  df_time_sub <- make_time_sequence_data(df, time_bin_size = tb_size, predictor_columns = "Condition", aois = "AOI1", summarize_by = "Participant")
  
  alpha = .05/num_time_bins
  num_sub = length(unique((df_time_sub$ParticipantName)))
  threshold_t = qt(p = 1 - alpha/2, 
                   df = num_sub-1)
  
  cl_dat <- make_time_cluster_data(data = df_time_sub, predictor_column = "Condition", test = "t.test", threshold = threshold_t, quiet = TRUE, formula = Elog ~ Condition)
  if ( nrow(get_time_clusters(cl_dat))>0 ) {
    cl_anal <- analyze_time_clusters(cl_dat, within_subj = FALSE, samples = n_bootstrap_samples, parallel = TRUE)
    cl_clusts <- get_time_clusters(cl_anal)
    cl_clusts$NumBins <- with(cl_clusts, (EndTime - StartTime)/tb_size)
    cl_sig_clusts <- filter(cl_clusts, cl_clusts$Probability<.05)
    out <- with(cl_sig_clusts, 
              data.frame(
                N_subjects = N_subjects,
                N_items = N_items,
                pref = pref, 
                avg_rt = mean(df$RT, na.rm=TRUE),
                rt_lower_quart = quantile(df$RT, probs = .25),
                rt_upper_quart = quantile(df$RT, probs = .75),
                false_alarm_bins = sum((EndTime[EndTime < (effect_start-tb_size) & StartTime < (effect_start-tb_size)] - StartTime[EndTime < (effect_start-tb_size) & StartTime < (effect_start-tb_size)])/tb_size, ((effect_start-tb_size)-StartTime[EndTime >= (effect_start-tb_size) & StartTime < (effect_start-tb_size)])/tb_size),
                correct_discovery_bins = sum((EndTime[StartTime >= (effect_start-tb_size)] - StartTime[StartTime >= (effect_start-tb_size)])/tb_size, (EndTime[StartTime < (effect_start-tb_size) & EndTime > (effect_start-tb_size)]-(effect_start-tb_size))/tb_size),
                first_discovery_time = StartTime[EndTime >= (effect_start-tb_size)][1],
                n_clusters = nrow(get_time_clusters(cl_dat))
              )  
        )
  } else {
    out <- data.frame(
                N_subjects = N_subjects,
                N_items = N_items,
                pref = pref, 
                avg_rt = mean(df$RT, na.rm=TRUE),
                rt_lower_quart = quantile(df$RT, probs = .25),
                rt_upper_quart = quantile(df$RT, probs = .75),
                false_alarm_bins = 0,
                correct_discovery_bins = 0,
                first_discovery_time = NA,
                n_clusters = 0
            )
  }
  # end analysis
  
  # begin return data
  return(out)
  # end return data
  
}

# merge results into dataframe
clusters_bonf <- cbind(clusters_bonf, clusters_bonf_results)
```

### FDR correction:

```{r}
clusters_fdr <- data.frame(
                  test = 'clusters_fdr',
                  correction_factor = fdr_correction,
                  effective_alpha = .05/fdr_correction
              )

clusters_fdr_results <- foreach(N_subjects=N_subjects, .combine=rbind) %:%
                   foreach(N_items=N_items, .combine=rbind) %:%
                   foreach(pref=pref, .combine=rbind) %dopar% {
  
  # begin analysis
  df <- simulate_eyetrackingr_data(num_participants = N_subjects, num_items_per_condition = N_items, pref = pref, pref_window = c(effect_start,trial_length))
  df_time_sub <- make_time_sequence_data(df, time_bin_size = tb_size, predictor_columns = "Condition", aois = "AOI1", summarize_by = "Participant")
  
  alpha = .05/fdr_correction
  num_sub = length(unique((df_time_sub$ParticipantName)))
  threshold_t = qt(p = 1 - alpha/2, 
                   df = num_sub-1)
  
  cl_dat <- make_time_cluster_data(data = df_time_sub, predictor_column = "Condition", test = "t.test", threshold = threshold_t, quiet = TRUE, formula = Elog ~ Condition)
  if ( nrow(get_time_clusters(cl_dat))>0 ) {
    cl_anal <- analyze_time_clusters(cl_dat, within_subj = FALSE, samples = n_bootstrap_samples, parallel = TRUE)
    cl_clusts <- get_time_clusters(cl_anal)
    cl_clusts$NumBins <- with(cl_clusts, (EndTime - StartTime)/tb_size)
    cl_sig_clusts <- filter(cl_clusts, cl_clusts$Probability<.05)
    out <- with(cl_sig_clusts, 
              data.frame(
                N_subjects = N_subjects,
                N_items = N_items,
                pref = pref, 
                avg_rt = mean(df$RT, na.rm=TRUE),
                rt_lower_quart = quantile(df$RT, probs = .25),
                rt_upper_quart = quantile(df$RT, probs = .75),
                false_alarm_bins = sum((EndTime[EndTime < (effect_start-tb_size) & StartTime < (effect_start-tb_size)] - StartTime[EndTime < (effect_start-tb_size) & StartTime < (effect_start-tb_size)])/tb_size, ((effect_start-tb_size)-StartTime[EndTime >= (effect_start-tb_size) & StartTime < (effect_start-tb_size)])/tb_size),
                correct_discovery_bins = sum((EndTime[StartTime >= (effect_start-tb_size)] - StartTime[StartTime >= (effect_start-tb_size)])/tb_size, (EndTime[StartTime < (effect_start-tb_size) & EndTime > (effect_start-tb_size)]-(effect_start-tb_size))/tb_size),
                first_discovery_time = StartTime[EndTime >= (effect_start-tb_size)][1],
                n_clusters = nrow(get_time_clusters(cl_dat))
              )  
        )
  } else {
    out <- data.frame(
                N_subjects = N_subjects,
                N_items = N_items,
                pref = pref, 
                avg_rt = mean(df$RT, na.rm=TRUE),
                rt_lower_quart = quantile(df$RT, probs = .25),
                rt_upper_quart = quantile(df$RT, probs = .75),
                false_alarm_bins = 0,
                correct_discovery_bins = 0,
                first_discovery_time = NA,
                n_clusters = 0
            )
  }
  # end analysis
  
  # begin return data
  return(out)
  # end return data
  
}

# merge results into dataframe
clusters_fdr <- cbind(clusters_fdr, clusters_fdr_results)
```

# Summarise Simulation results

```{r}
all_tests <- rbind(
              t_tests,
              t_tests_bonf,
              t_tests_fdr,
              boot_splines,
              boot_splines_bonf,
              boot_splines_fdr,
              clusters,
              clusters_bonf,
              clusters_fdr
            )

summary <- all_tests %>%
           group_by(test, correction_factor, effective_alpha, N_subjects, N_items, pref) %>%
           summarise(
             CorrectDiscoveryRate = mean(correct_discovery_bins) / (num_time_bins/2),
             NoDiscoveryRate = mean(correct_discovery_bins == 0),
             MeanFirstDivergence = mean(first_discovery_time, na.rm=TRUE),
             FalseDiscoveryRate = mean(false_alarm_bins) / (num_time_bins/2),
             FamilyWiseErrorRate = mean(false_alarm_bins != 0),
             NumClusters = mean(n_clusters)
           ) %>%
           ungroup() %>%
           mutate(
             pref = factor(pref)
           )

summary_long <- summary %>%
           tidyr::gather('Measure', 'Value', CorrectDiscoveryRate, NoDiscoveryRate, FalseDiscoveryRate, FamilyWiseErrorRate,NumClusters)

# visualize measures in a high power (N=90) design
ggplot(subset(summary_long, Measure != 'NumClusters'), aes(x=as.factor(N_subjects), y=Value, fill=test)) +
                  scale_fill_manual(values=c('#000000','#525252','#969696',
                                             '#3f007d','#6a51a3','#9e9ac8',
                                             '#d94801','#fd8d3c','#fdd0a2')) +
                  scale_x_discrete(name='Number of Subjects') +
                  scale_y_continuous(name='Proportion') +
                  stat_summary(fun.y='mean', geom='bar', position=position_dodge()) +
                  # geom_line(size=2) + # uncomment when we have enough tests for it to be clean
                  facet_grid(Measure~pref) +
                  theme_bw()

# visualize # of clusters
ggplot(subset(summary_long, Measure == 'NumClusters'), aes(x=as.factor(test), y=Value)) +
                  stat_summary(fun.y='mean', geom='bar')

# visualize number of diverged bins

# overall summary
summary_overall <- summary %>%
                   group_by(test, correction_factor, effective_alpha) %>%
                   summarise(
                     CorrectDiscoveryRate = mean(CorrectDiscoveryRate),
                     NoDiscoveryRate = mean(NoDiscoveryRate),
                     MeanFirstDivergence = mean(MeanFirstDivergence,na.rm=TRUE),
                     FalseDiscoveryRate = mean(FalseDiscoveryRate),
                     FamilyWiseErrorRate = mean(FamilyWiseErrorRate),
                     NumClusters = mean(NumClusters)
                   ) %>% 
                   ungroup() %>%
                   as.data.frame()

summary_overall
```